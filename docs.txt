return code
100 friend related
    110 + return code

    101 add friend success
    102 add friend failed
    103 already friend
    104 friend req has send, wait to accept
200 message related
    210 + return code

    201 send message success
    202 target is not friend
    203 send message failed
    204 target is offline
300 target status
    target come online
    target offline
    
    
400 toxcore related
    401 toxcore offline
    402 connect OK
    
分包过程
    这个信息传输应该是序列化的，所以只要在每个包上加一个id就行了，就用uuid
    
    
整体通信过程设计
    显然这个需要双向的即时通信。
    toxcore 和 node.js 通信还是采用1问1答的方式。但是tcp是客户端和服务器的模型，怎么整呢。
    还是用tcp通信，但是通信一直都不关。这样设计起来太费事了
    还是采用之前设想的轮询的方式？但是这样数据量大的时候效率太低了
    或者再另开一个tcp连接，要么用udp重新设计一种模式
    再开一个连接显得太臃肿了
    
    网络结构
    
    A机 PORT a                                               B机 PORT b
        |                                                       |
     node.js binding                                        node.js binding
        |                                                       |
        |------->A机 toxcore -->------>------>--->  B机 toxcore--|
        
    node.js 在这个过程中完全不是必须的，引入的原因就是增加开发效率，但是现在通信过程过于复杂，反倒减少效率了。
    但是分包，和其他比较复杂的处理C感觉会很难做
    应该先把底层构建好了，再在上面构建应用层
    这就是一个双向通信的问题
    node 和本地还是应该长连，效率上会更高，底层的基本连接toxcore完全对node进行屏蔽，比如说加好友，判断在线，连接的建立
    过程。toxcore对上层暴露的就是一个每次传输最大1024 byte的通道
    
    整个过程
    1.node 启动，启动完成后启动本地toxcore 这时向 toxcore 传入 tox节点信息，远程ID和远程端口，如果没有远程ID和端口则进入监听模式
    2.node 建立本地tcp监听端口A
    3.toxcore 启动完成之后连接A端口，向node发送信息说明已经准备OK
    4.node 监听本地端口B， 等待本地程序连接B
    
    请求连接端就是如此，下面是接收连接端的过程
    1.node 启动只传入tox节点的信息
    2.node 建立本地tcp监听端口C
    3.toxcore 开始监听信息，同时和node的C端口建立连接
    4.toxcore 和对方建立连接，收到信息后将信息转发给node
    
    底层toxcore的建立连接过程
    请求连接端
    1.toxcore启动连接入tox网络
        载入以前的配置只保留ID信息，friend全部删除
    2.发送add friend 请求
    3.等待对方connect status变化
    4.发送握手信息确认连接
    5.连接成功
    
    接收连接端
    1.toxcore启动进入tox网络
    2.等待对方请求
    3.收到请求后将对方加入friend
    4.等待对方握手信息  #这是主动向对方发握手包速度会不会更快些
    
    具体的设计
    toxcore 方面
    toxcore开两个线程，一个用来监听本地端口，一个用来监听tox的消息
    主要的处理过程放在tox的线程里面。任务全部都做成队列。这是一个不错的设计
    toxcore主要做两件事情， 一个是接收node的信息，直接传给对方
    来自node的消息在另一个tcp的进程里面完成，远端信息的添加在回调函数里面完成，这样的话回调函数应该也另开一个线性，因为消息处理可能会阻塞
    另一个是接收对方的信息直接传给node
    这里需要两个队列，一个来自node的消息队列，另一个是来自对方的消息队列
    这里不采用一问一答的形式，toxcore不会直接对本地的node信息进行回复，确认连接应该在更底层和更高层进行
    
    主要的處理過程
    對於請求端，node向本地toxcore發送和接收信息就不用包含對方的ID信息
    對於接受請求端，就要包含ID信息，toxcore端根據請求的ID向對方發送指定信息
    
        
        
        341CCFBC
        C4D41C5B
        3AB89E31
        E7561C5D
        37E201D5
        DDBFA7AF
        C6B4EDD2
        D6A82F4B
        7D06A2ED
        3DE4
        341CCFBC
        C4D41C5B
        3AB89E31
        E7561C5D
        37E21D5D
        DBFA7AFC
        6B4EDD2D
        6A82F4B4
        01
        341CCFBCC4D41C5B3AB89E31E7561C5D37E201D5DDBFA7AFC6B4EDD2D6A82F4B7D06A2ED3DE4
        FC2BD4C82FC377648EB0CB29A6A7BA3356697F9848DFA2CD93B4219311C40722
        FC2BD4C82FC377648EB0CB29A6A7BA3356697F9848DFA2CD93B4219311C40722464332424434
        9D9D799FAD4EEF3EB2B56568DC61B3B49DF223C83FF1566739C7B936B792F344247F52397BB7
        9D9D799FAD4EEF3EB2B56568DC61B3B49DF223C83FF1566739C7B936B792F344
bugs
    第一次握手連接時總是不成功(fixed)
        原因是我把接受连接设计成阻塞的模式却忘记开新的线程了，导致收到连接请求后就一直堵在那里
    連接成功後信息總是會重複(fixed)
        这是幻觉，只发送了一遍，但是在显示的时候重复显示了
    handshake明明收到了,卻會被判定成沒收到(fixed)
        字符串处理的问题，最后一位没有加终止符，确切的说是加了，但是被后来的字符串给覆盖了
    传递的端口号值不对(fixed)
        端口号设置成uint8了
        
        
可能的问题
    内存泄露
    
    
    
    
由于是连续的传递信息，为了区分传进来的不同次数应该进行手动分割，
有没有必要进行分割
好像完全没有必要进行分包
直接传就行
试试直接传吧 直传测试成功

本地连接还是应该开两个，一个专门用来传数据另一个用来传控制指令,必须要考虑同时有多个连接的情况，这个太普遍了

还是全部都用C比较好，这样通信过程就简单多了
但是开发效率会很低，一个list都要写挺长时间，字符串处理更头疼

怎么分成一次一次的
先读控制用socket
如果读到 READ：user client id
那么就开始读数据socket，直到读的没数据 这样就算一次
所以node在传数据的时候要有独占性，要加锁，加消息队列
读完之后在控制端写入 read:over

控制端的指令采用一问一答的形式，在对方回答之前不能写入新的指令


sock的生命周期

初始化tox连接
// 还要考虑不同的电脑连接同一个服务器的情况

socket的创建

创建sock之前的准备工作
初始化tox连接
    tox 发送INIT_REQ 指令到远端
    如果远程连接建立成功
        在node上存储以下信息
            {
                clientID: user client id
                ip address:
                port:
            }
        如果数据已经存在则可以进行覆盖，每个clientID只能同时连接一个特定端口
        
node 创建sock服务器
本地sock连接到 node的时候
node 创建一个对象
{
    from: client id, can be ctrl or data
    sock:sock对象
    uuid:uuid
}
设置sock的ondata事件回调，在on data的时候就向把以下信息加入到消息队列里面 // 这里是node收到了本地的sock的信息
{
    uuid
    data
    sock
}
node 定时的把消息队列中的消息利用上面设计的方法发送给toxcore
toxcore 收到消息之后把消息加入到消息队列，主循环内对消息队列进行处理
toxcore 收到远端信息的时候，把消息处理成下面的格式 

在tox网络传递的内容
{
    uuid：socket uuid
    data: raw data  <512>
}

远程toxcore接收到信息
处理成以下格式

远端toxcore接收到后加上发送者ID
{
    from: client id
    uuid: socket uuid
    data: raw data <512>
}
把消息加入发送队列里面
利用之前设计的方法发送给node

node 收到消息之后，如果uuid对应的记录已经存在，就把raw data发给对应的sock
如果不存在就创建一个sock连接到对应端口，把raw data发送到指定对象
同时创建以下对象
{
    from: client id
    uuid:
    sock
}


server 端收到本地信息的处理过程
首先sock client和对应的记录都是已经存在了
找到对应的记录

按照之前设计的方法发送信息给toxcore
控制端这时要包含两个信息
    uuid
    target client id
    
发送的过程都是一样的，只不过这里要根据target id 发送给不同的对象

请求端接收到消息的时候，处理成的形式添加到发送队列
    uuid 
    data raw data<512>



sock的消灭
    正常的退出
        本地sock client关闭
        node 检测到本地sock关闭，删除sock记录
        控制端发送以下信息给toxcore
            close:uuid
            target id: client id
        toxcore 处理成以下格式
            {
                target_id:
                uuid:
                cmd:close
            }
        加入消息队列
        
        远端toxcore收到消息之后把消息发送个node，node关闭对应sock，同时删除sock对应的记录
        
    非正常退出
        这个还真没办法处理，不过危害也不大，只不过残留了一些资源而已
        有办法，这个实际上就是一方tox_send_message突然就失败了。
        
        如果是请求端无法连接
        toxcore 发送以下信息到node
        {
            target_id:
            uuid:
            cmd:close
        }
        
        如果是服务端无法连接，采用一样的方法
        
        
指令控制用sock 发送 

本地新建sock client 连接到 本地 node服务器，本地生成一个sock的 uuid


sock 发送信息时 在信息内容里面加上自己的uuid 远端如果没有这个记录就创建一个， 远端node就创建一个新的sock clinet，


一个socket包的格式
在tox网络传递的内容
{
    uuid：socket uuid
    data: raw data  <512>
}

远端toxcore接收到后加上发送者ID
{
    from: client id
    uuid: socket uuid
    data: raw data <512>
}
// 发送消息的时候不是直接发送，把消息存到消息队列里面，在tox主循环内发送


计划步骤
1.完成本地的通信过程(complete)
2.完成tox的通信过程(complete)
    关闭本地sock出问题了,总算关上了，有shutdown就行了
3.整体的通信过程

还是完全用C来写得了，省得这么费事的处理通信过程


重新设计

先分成客户端和服务器两个独立的程序，最后再合并成一个程序

客户端程序
    初始化tox连接
    当本地客户端连接到本地服务器时
    创建一个线程进行处理 这个线程就是在不停的read
    把收到的消息添加到消息队列
    {
        uuid
        sock
        data
        length
    }
    
    tox主循环对数据进行处理
    
服务器程序
    初始化连接
    尝试连接到制定端口
    如果失败就返回 ERROR
    成功就创建一个结构体数组
    {
        from: id
        ip
        port
    }
    
crash 原因
在还没创建好对象的时候就开始写入程序了
原来如此，由于参数传递是用的指针，但是这个参数是局部变量，所以有时候参数就会传错(fixed)
这回是本地端崩了
    原因是收到了奇怪的没内容的信息
    可能是json在数据传输的时候加了\0在数据里面
    发送非字符串数据时，传输数据为空
    传输效率太低了
    
数据在传输过程中会被修改，可能是类型转换时的问题
数据的发送不采用json了，自定义格式

36B uuid
1B  data or command 
    0 data
    1 create sock
    2 close sock
2B  data length
1024B data context

经过变换会少一个字节，但是看起来又是一样的
    
    
    
253,254,255\n
,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255\n
,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,
240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255\n,0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238

经过测试发送端没有发现异常
原来有时会发送失败

内存的管理方式
要想到一种比较完善的内存管理原则
先分析情况
主要的问题在于结构体内的指针所指向的变量，由于这个结构体要在不同的函数之间进行传递，所以指针指向的不能是局域变量
这个指针所指向的对象应该是和结构体同样的生命周期，所以这里应该把指针的指向内容复制一份

或者可以采用另外一种方式，结构体只是存指针，队列操作完全不包含内部元素的内存分配。free和malloc全部是由外部进行的。这样的话封装性就不好
还是采用第一种方式比较好，也不用考虑作用域的问题了

基本原则就是malloc 和free要在同一个域中，比如在同一个函数之中或者在同一类成套的函数之中，比如向链表插入值，这时是malloc，删除这个值就要
free

这样看来js和java的内存管理逻辑还是最清楚，全部都是引用，只要引用不为零就不回收